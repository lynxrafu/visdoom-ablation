{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViZDoom Ablation Study - Results Analysis\n",
    "\n",
    "**IEEE Report Data Generation**\n",
    "\n",
    "Analyzes all 33 experiment runs and generates tables/plots for the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Check if on Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    RESULTS_DIR = Path('/content/drive/MyDrive/vizdoom-ablation-results')\n",
    "else:\n",
    "    RESULTS_DIR = Path('results')\n",
    "\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"Exists: {RESULTS_DIR.exists()}\")\n",
    "\n",
    "# List date folders\n",
    "if RESULTS_DIR.exists():\n",
    "    date_folders = [f.name for f in RESULTS_DIR.iterdir() if f.is_dir() and not f.name.startswith('.')]\n",
    "    print(f\"Date folders: {sorted(date_folders)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_results(results_dir):\n",
    "    \"\"\"Load all experiment metadata and summaries.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for date_folder in sorted(results_dir.iterdir()):\n",
    "        if not date_folder.is_dir() or date_folder.name.startswith('.'):\n",
    "            continue\n",
    "        if not date_folder.name.startswith('202'):  # Skip non-date folders\n",
    "            continue\n",
    "            \n",
    "        for run_folder in sorted(date_folder.iterdir()):\n",
    "            if not run_folder.is_dir():\n",
    "                continue\n",
    "                \n",
    "            meta_file = run_folder / 'metadata.json'\n",
    "            summary_file = run_folder / 'summary.json'\n",
    "            csv_file = run_folder / 'training_log.csv'\n",
    "            \n",
    "            if not meta_file.exists():\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                with open(meta_file) as f:\n",
    "                    meta = json.load(f)\n",
    "                \n",
    "                # Load summary if exists\n",
    "                summary = {}\n",
    "                if summary_file.exists():\n",
    "                    with open(summary_file) as f:\n",
    "                        summary = json.load(f)\n",
    "                \n",
    "                # Load training CSV if exists\n",
    "                training_df = None\n",
    "                if csv_file.exists():\n",
    "                    training_df = pd.read_csv(csv_file)\n",
    "                \n",
    "                # Categorize experiment\n",
    "                agent = meta.get('agent_type', 'unknown')\n",
    "                n_step = meta.get('n_step', 1)\n",
    "                per = meta.get('buffer_prioritized', False)\n",
    "                \n",
    "                if agent == 'dqn' and n_step == 1 and not per:\n",
    "                    phase = 'Phase1'\n",
    "                    method = 'DQN'\n",
    "                elif agent == 'deep_sarsa':\n",
    "                    phase = 'Phase1'\n",
    "                    method = 'DeepSARSA'\n",
    "                elif n_step > 1:\n",
    "                    phase = 'Phase2'\n",
    "                    method = f'DQN_n{n_step}'\n",
    "                elif per:\n",
    "                    phase = 'Phase3a'\n",
    "                    method = 'DQN_PER'\n",
    "                elif agent == 'ddqn':\n",
    "                    phase = 'Phase3b'\n",
    "                    method = 'DDQN'\n",
    "                elif 'dueling' in agent:\n",
    "                    phase = 'Phase3c'\n",
    "                    method = 'Dueling_DDQN'\n",
    "                else:\n",
    "                    phase = 'Other'\n",
    "                    method = agent\n",
    "                \n",
    "                results.append({\n",
    "                    'phase': phase,\n",
    "                    'method': method,\n",
    "                    'scenario': meta.get('scenario_short', meta.get('scenario', 'unknown')),\n",
    "                    'seed': meta.get('seed', 0),\n",
    "                    'n_step': n_step,\n",
    "                    'per': per,\n",
    "                    'lr': meta.get('learning_rate', 0.0001),\n",
    "                    'gamma': meta.get('gamma', 0.99),\n",
    "                    # Correct field names from summary.json\n",
    "                    'final_reward': summary.get('final_avg_reward_100', None),\n",
    "                    'best_eval': summary.get('best_eval_reward', None),\n",
    "                    'total_episodes': summary.get('num_episodes', meta.get('num_episodes', None)),\n",
    "                    'training_hours': summary.get('total_time_hours', None),\n",
    "                    'run_dir': str(run_folder),\n",
    "                    'run_name': meta.get('run_name', run_folder.name),\n",
    "                    'training_df': training_df,\n",
    "                    'date': date_folder.name\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {run_folder.name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load all results\n",
    "all_results = load_all_results(RESULTS_DIR)\n",
    "print(f\"\\nLoaded {len(all_results)} experiment runs\")\n",
    "\n",
    "# Convert to DataFrame (without training_df column)\n",
    "df = pd.DataFrame([{k:v for k,v in r.items() if k != 'training_df'} for r in all_results])\n",
    "\n",
    "if len(df) > 0:\n",
    "    print(f\"\\nPhases: {df['phase'].unique()}\")\n",
    "    print(f\"Methods: {df['method'].unique()}\")\n",
    "    print(f\"Scenarios: {df['scenario'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Experiment Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full results table\n",
    "print(\"=\" * 80)\n",
    "print(\"ALL EXPERIMENTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "display_cols = ['phase', 'method', 'scenario', 'seed', 'final_reward', 'best_eval', 'training_hours']\n",
    "print(df[display_cols].sort_values(['phase', 'method', 'scenario', 'seed']).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY BY METHOD AND SCENARIO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary = df.groupby(['phase', 'method', 'scenario']).agg({\n",
    "    'seed': 'count',\n",
    "    'final_reward': ['mean', 'std'],\n",
    "    'best_eval': ['mean', 'std'],\n",
    "    'training_hours': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "summary.columns = ['n_runs', 'final_mean', 'final_std', 'best_mean', 'best_std', 'hours']\n",
    "print(summary.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs per phase\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RUNS PER PHASE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "phase_summary = df.groupby('phase').agg({\n",
    "    'seed': 'count',\n",
    "    'training_hours': 'sum'\n",
    "}).round(2)\n",
    "phase_summary.columns = ['runs', 'total_hours']\n",
    "\n",
    "for phase, row in phase_summary.iterrows():\n",
    "    print(f\"{phase:10} | {int(row['runs']):2} runs | {row['total_hours']:.1f} hours\")\n",
    "\n",
    "print(f\"{'TOTAL':10} | {len(df):2} runs | {df['training_hours'].sum():.1f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Phase 1: DQN vs Deep SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1 detailed comparison\n",
    "phase1 = df[df['phase'] == 'Phase1'].copy()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 1: DQN vs Deep SARSA (Off-policy vs On-policy)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Pivot table\n",
    "phase1_pivot = phase1.pivot_table(\n",
    "    values=['final_reward', 'best_eval'],\n",
    "    index='scenario',\n",
    "    columns='method',\n",
    "    aggfunc=['mean', 'std']\n",
    ").round(2)\n",
    "\n",
    "print(phase1_pivot.to_string())\n",
    "\n",
    "# Winner per scenario\n",
    "print(\"\\n--- Winner by Scenario ---\")\n",
    "for scenario in ['Basic', 'TakeCover', 'Deathmatch']:\n",
    "    dqn_data = phase1[(phase1['method'] == 'DQN') & (phase1['scenario'] == scenario)]\n",
    "    sarsa_data = phase1[(phase1['method'] == 'DeepSARSA') & (phase1['scenario'] == scenario)]\n",
    "    \n",
    "    dqn_best = dqn_data['best_eval'].mean() if len(dqn_data) > 0 else float('-inf')\n",
    "    sarsa_best = sarsa_data['best_eval'].mean() if len(sarsa_data) > 0 else float('-inf')\n",
    "    \n",
    "    winner = 'DQN' if dqn_best > sarsa_best else 'DeepSARSA'\n",
    "    diff = dqn_best - sarsa_best\n",
    "    print(f\"{scenario:12} | DQN: {dqn_best:8.2f} | SARSA: {sarsa_best:8.2f} | Winner: {winner} ({diff:+.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1 Learning Curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "scenarios = ['Basic', 'TakeCover', 'Deathmatch']\n",
    "colors = {'DQN': '#1f77b4', 'DeepSARSA': '#ff7f0e'}\n",
    "\n",
    "for idx, scenario in enumerate(scenarios):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for method in ['DQN', 'DeepSARSA']:\n",
    "        method_results = [r for r in all_results \n",
    "                         if r['phase'] == 'Phase1' \n",
    "                         and r['method'] == method \n",
    "                         and r['scenario'] == scenario\n",
    "                         and r['training_df'] is not None]\n",
    "        \n",
    "        if not method_results:\n",
    "            continue\n",
    "            \n",
    "        # Collect all rewards\n",
    "        all_rewards = []\n",
    "        for r in method_results:\n",
    "            tdf = r['training_df']\n",
    "            if 'episode' in tdf.columns and 'reward' in tdf.columns:\n",
    "                rewards = tdf.set_index('episode')['reward']\n",
    "                all_rewards.append(rewards)\n",
    "        \n",
    "        if all_rewards:\n",
    "            combined = pd.concat(all_rewards, axis=1)\n",
    "            mean_reward = combined.mean(axis=1)\n",
    "            std_reward = combined.std(axis=1)\n",
    "            \n",
    "            # Rolling average for smoothing\n",
    "            window = 50\n",
    "            mean_smooth = mean_reward.rolling(window, min_periods=1).mean()\n",
    "            std_smooth = std_reward.rolling(window, min_periods=1).mean()\n",
    "            \n",
    "            ax.plot(mean_smooth.index, mean_smooth.values, \n",
    "                   label=method, color=colors[method], linewidth=1.5)\n",
    "            ax.fill_between(mean_smooth.index, \n",
    "                           mean_smooth.values - std_smooth.values,\n",
    "                           mean_smooth.values + std_smooth.values,\n",
    "                           alpha=0.2, color=colors[method])\n",
    "    \n",
    "    ax.set_title(f'{scenario}', fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Reward')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Phase 1: DQN vs Deep SARSA Learning Curves (3 seeds avg)', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('phase1_learning_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: phase1_learning_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Phase 2: N-Step Ablation (n=1 vs n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Compare DQN n=1 (baseline) vs DQN n=3\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 2: N-STEP ABLATION (TD/MC Bridge)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Baseline: DQN with n=1 from Phase 1\n",
    "baseline_n1 = df[(df['method'] == 'DQN') & (df['phase'] == 'Phase1')].copy()\n",
    "baseline_n1['method'] = 'DQN_n1'\n",
    "\n",
    "# N-step: DQN with n=3 from Phase 2\n",
    "nstep_n3 = df[df['phase'] == 'Phase2'].copy()\n",
    "\n",
    "# Combine\n",
    "nstep_compare = pd.concat([baseline_n1, nstep_n3])\n",
    "\n",
    "# Pivot\n",
    "nstep_pivot = nstep_compare.pivot_table(\n",
    "    values='best_eval',\n",
    "    index='scenario',\n",
    "    columns='method',\n",
    "    aggfunc=['mean', 'std', 'count']\n",
    ").round(2)\n",
    "\n",
    "print(nstep_pivot.to_string())\n",
    "\n",
    "# Impact analysis\n",
    "print(\"\\n--- N-step Impact (n=3 vs n=1) ---\")\n",
    "for scenario in ['Basic', 'TakeCover', 'Deathmatch']:\n",
    "    n1 = nstep_compare[(nstep_compare['method'] == 'DQN_n1') & (nstep_compare['scenario'] == scenario)]['best_eval'].mean()\n",
    "    n3 = nstep_compare[(nstep_compare['method'] == 'DQN_n3') & (nstep_compare['scenario'] == scenario)]['best_eval'].mean()\n",
    "    \n",
    "    if pd.notna(n1) and pd.notna(n3):\n",
    "        diff = n3 - n1\n",
    "        pct = (diff / abs(n1)) * 100 if n1 != 0 else 0\n",
    "        verdict = \"BETTER\" if diff > 0 else \"WORSE\"\n",
    "        print(f\"{scenario:12} | n=1: {n1:8.2f} | n=3: {n3:8.2f} | Diff: {diff:+8.2f} ({pct:+6.1f}%) {verdict}\")\n",
    "    else:\n",
    "        print(f\"{scenario:12} | Data missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Phase 3: Extensions (PER, DDQN, Dueling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3: All extensions vs baseline\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 3: DQN EXTENSIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Baseline DQN\n",
    "baseline = df[(df['method'] == 'DQN') & (df['phase'] == 'Phase1')].copy()\n",
    "baseline['method'] = 'Baseline_DQN'\n",
    "\n",
    "# All Phase 3\n",
    "phase3 = df[df['phase'].str.startswith('Phase3')].copy()\n",
    "\n",
    "# Combine\n",
    "ext_compare = pd.concat([baseline, phase3])\n",
    "\n",
    "# Pivot for best_eval\n",
    "ext_pivot = ext_compare.pivot_table(\n",
    "    values='best_eval',\n",
    "    index='scenario',\n",
    "    columns='method',\n",
    "    aggfunc='mean'\n",
    ").round(2)\n",
    "\n",
    "print(\"Mean Best Eval by Method:\")\n",
    "print(ext_pivot.to_string())\n",
    "\n",
    "# Improvement over baseline\n",
    "print(\"\\n--- Improvement over Baseline DQN ---\")\n",
    "for scenario in ['Basic', 'TakeCover']:\n",
    "    print(f\"\\n{scenario}:\")\n",
    "    base_val = ext_pivot.loc[scenario, 'Baseline_DQN'] if scenario in ext_pivot.index else None\n",
    "    \n",
    "    if base_val is None:\n",
    "        continue\n",
    "        \n",
    "    for method in ['DQN_PER', 'DDQN', 'Dueling_DDQN']:\n",
    "        if method in ext_pivot.columns and scenario in ext_pivot.index:\n",
    "            val = ext_pivot.loc[scenario, method]\n",
    "            if pd.notna(val):\n",
    "                diff = val - base_val\n",
    "                pct = (diff / abs(base_val)) * 100 if base_val != 0 else 0\n",
    "                print(f\"  {method:15} | {val:8.2f} | vs baseline: {diff:+8.2f} ({pct:+6.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart: All methods comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "scenarios = ['Basic', 'TakeCover']\n",
    "method_order = ['Baseline_DQN', 'DQN_n3', 'DQN_PER', 'DDQN', 'Dueling_DDQN']\n",
    "method_labels = ['DQN\\n(baseline)', 'DQN\\nn=3', 'DQN\\n+PER', 'DDQN', 'Dueling\\n+DDQN']\n",
    "colors = ['#7f7f7f', '#1f77b4', '#2ca02c', '#ff7f0e', '#d62728']\n",
    "\n",
    "# Combine all data\n",
    "all_compare = pd.concat([baseline, nstep_n3, phase3])\n",
    "\n",
    "for idx, scenario in enumerate(scenarios):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    means = []\n",
    "    stds = []\n",
    "    valid_labels = []\n",
    "    valid_colors = []\n",
    "    \n",
    "    for i, method in enumerate(method_order):\n",
    "        data = all_compare[(all_compare['method'] == method) & (all_compare['scenario'] == scenario)]['best_eval']\n",
    "        if len(data) > 0:\n",
    "            means.append(data.mean())\n",
    "            stds.append(data.std() if len(data) > 1 else 0)\n",
    "            valid_labels.append(method_labels[i])\n",
    "            valid_colors.append(colors[i])\n",
    "    \n",
    "    if means:\n",
    "        x = range(len(means))\n",
    "        bars = ax.bar(x, means, yerr=stds, capsize=4, color=valid_colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(valid_labels, fontsize=9)\n",
    "        ax.set_ylabel('Best Eval Reward')\n",
    "        ax.set_title(f'{scenario}', fontsize=11, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, mean in zip(bars, means):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, \n",
    "                   f'{mean:.0f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.suptitle('All Methods Comparison (Best Eval Reward)', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('all_methods_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: all_methods_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. IEEE Report Tables (LaTeX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LATEX TABLES FOR IEEE REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Table 1: Phase 1\n",
    "print(\"\\n% ===== TABLE 1: DQN vs Deep SARSA =====\")\n",
    "print(r\"\\begin{table}[h]\")\n",
    "print(r\"\\centering\")\n",
    "print(r\"\\caption{Phase 1: DQN vs Deep SARSA Comparison}\")\n",
    "print(r\"\\label{tab:phase1}\")\n",
    "print(r\"\\begin{tabular}{|l|l|c|c|}\")\n",
    "print(r\"\\hline\")\n",
    "print(r\"\\textbf{Scenario} & \\textbf{Method} & \\textbf{Best Eval} & \\textbf{Final Reward} \\\\\")\n",
    "print(r\"\\hline\")\n",
    "\n",
    "for scenario in ['Basic', 'TakeCover', 'Deathmatch']:\n",
    "    for method in ['DQN', 'DeepSARSA']:\n",
    "        data = phase1[(phase1['scenario'] == scenario) & (phase1['method'] == method)]\n",
    "        if len(data) > 0:\n",
    "            best_mean = data['best_eval'].mean()\n",
    "            best_std = data['best_eval'].std()\n",
    "            final_mean = data['final_reward'].mean() if data['final_reward'].notna().any() else float('nan')\n",
    "            final_std = data['final_reward'].std() if data['final_reward'].notna().any() else float('nan')\n",
    "            \n",
    "            best_str = f\"${best_mean:.1f} \\\\pm {best_std:.1f}$\" if pd.notna(best_std) else f\"${best_mean:.1f}$\"\n",
    "            final_str = f\"${final_mean:.1f} \\\\pm {final_std:.1f}$\" if pd.notna(final_std) else \"-\"\n",
    "            \n",
    "            print(f\"{scenario} & {method} & {best_str} & {final_str} \\\\\\\\\")\n",
    "    print(r\"\\hline\")\n",
    "\n",
    "print(r\"\\end{tabular}\")\n",
    "print(r\"\\end{table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 2: All Extensions\n",
    "print(\"\\n% ===== TABLE 2: All Extensions Comparison =====\")\n",
    "print(r\"\\begin{table}[h]\")\n",
    "print(r\"\\centering\")\n",
    "print(r\"\\caption{DQN Extensions Comparison (Best Eval Reward)}\")\n",
    "print(r\"\\label{tab:extensions}\")\n",
    "print(r\"\\begin{tabular}{|l|c|c|c|}\")\n",
    "print(r\"\\hline\")\n",
    "print(r\"\\textbf{Method} & \\textbf{Basic} & \\textbf{TakeCover} & \\textbf{Deathmatch} \\\\\")\n",
    "print(r\"\\hline\")\n",
    "\n",
    "all_compare = pd.concat([baseline, nstep_n3, phase3])\n",
    "method_names = {\n",
    "    'Baseline_DQN': 'DQN (baseline)',\n",
    "    'DQN_n3': 'DQN + n-step (n=3)',\n",
    "    'DQN_PER': 'DQN + PER',\n",
    "    'DDQN': 'Double DQN',\n",
    "    'Dueling_DDQN': 'Dueling + DDQN'\n",
    "}\n",
    "\n",
    "for method in ['Baseline_DQN', 'DQN_n3', 'DQN_PER', 'DDQN', 'Dueling_DDQN']:\n",
    "    row = [method_names.get(method, method)]\n",
    "    for scenario in ['Basic', 'TakeCover', 'Deathmatch']:\n",
    "        data = all_compare[(all_compare['method'] == method) & (all_compare['scenario'] == scenario)]['best_eval']\n",
    "        if len(data) > 0:\n",
    "            mean = data.mean()\n",
    "            std = data.std() if len(data) > 1 else 0\n",
    "            row.append(f\"${mean:.1f} \\\\pm {std:.1f}$\" if std > 0 else f\"${mean:.1f}$\")\n",
    "        else:\n",
    "            row.append(\"-\")\n",
    "    print(f\"{row[0]} & {row[1]} & {row[2]} & {row[3]} \\\\\\\\\")\n",
    "\n",
    "print(r\"\\hline\")\n",
    "print(r\"\\end{tabular}\")\n",
    "print(r\"\\end{table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "export_cols = ['phase', 'method', 'scenario', 'seed', 'n_step', 'per', \n",
    "               'final_reward', 'best_eval', 'training_hours', 'date', 'run_name']\n",
    "export_df = df[export_cols].sort_values(['phase', 'method', 'scenario', 'seed'])\n",
    "\n",
    "export_df.to_csv('all_results.csv', index=False)\n",
    "print(\"Exported: all_results.csv\")\n",
    "\n",
    "# Display\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPLETE RESULTS TABLE\")\n",
    "print(\"=\" * 80)\n",
    "display(export_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"KEY FINDINGS FOR IEEE REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate key metrics\n",
    "all_compare = pd.concat([baseline, nstep_n3, phase3])\n",
    "\n",
    "print(\"\\n1. PHASE 1: DQN vs Deep SARSA\")\n",
    "print(\"-\" * 40)\n",
    "for scenario in ['Basic', 'TakeCover', 'Deathmatch']:\n",
    "    dqn = phase1[(phase1['method'] == 'DQN') & (phase1['scenario'] == scenario)]['best_eval'].mean()\n",
    "    sarsa = phase1[(phase1['method'] == 'DeepSARSA') & (phase1['scenario'] == scenario)]['best_eval'].mean()\n",
    "    winner = 'DQN' if dqn > sarsa else 'DeepSARSA'\n",
    "    print(f\"   {scenario}: {winner} wins (DQN={dqn:.1f}, SARSA={sarsa:.1f})\")\n",
    "\n",
    "print(\"\\n2. PHASE 2: N-Step Impact\")\n",
    "print(\"-\" * 40)\n",
    "for scenario in ['Basic', 'TakeCover', 'Deathmatch']:\n",
    "    n1 = nstep_compare[(nstep_compare['method'] == 'DQN_n1') & (nstep_compare['scenario'] == scenario)]['best_eval'].mean()\n",
    "    n3 = nstep_compare[(nstep_compare['method'] == 'DQN_n3') & (nstep_compare['scenario'] == scenario)]['best_eval'].mean()\n",
    "    if pd.notna(n1) and pd.notna(n3):\n",
    "        verdict = \"IMPROVED\" if n3 > n1 else \"DEGRADED\"\n",
    "        print(f\"   {scenario}: n=3 {verdict} ({n3-n1:+.1f})\")\n",
    "\n",
    "print(\"\\n3. PHASE 3: Best Extension per Scenario\")\n",
    "print(\"-\" * 40)\n",
    "for scenario in ['Basic', 'TakeCover']:\n",
    "    scenario_data = all_compare[all_compare['scenario'] == scenario]\n",
    "    if len(scenario_data) > 0:\n",
    "        best = scenario_data.groupby('method')['best_eval'].mean().idxmax()\n",
    "        best_val = scenario_data.groupby('method')['best_eval'].mean().max()\n",
    "        print(f\"   {scenario}: {best} ({best_val:.1f})\")\n",
    "\n",
    "print(\"\\n4. TOTAL TRAINING TIME\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"   {df['training_hours'].sum():.1f} hours across {len(df)} experiments\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
