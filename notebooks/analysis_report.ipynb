{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViZDoom Ablation Study - Results Analysis\n",
    "\n",
    "**IEEE Report Data Generation**\n",
    "\n",
    "This notebook analyzes all experiment results and generates tables/plots for the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if on Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    RESULTS_DIR = Path('/content/drive/MyDrive/vizdoom-ablation-results')\n",
    "else:\n",
    "    RESULTS_DIR = Path('results')\n",
    "\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"Exists: {RESULTS_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all experiment results\n",
    "def load_all_results(results_dir):\n",
    "    \"\"\"Load all experiment metadata and summaries.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for date_folder in sorted(results_dir.iterdir()):\n",
    "        if not date_folder.is_dir() or date_folder.name.startswith('.'):\n",
    "            continue\n",
    "            \n",
    "        for run_folder in sorted(date_folder.iterdir()):\n",
    "            if not run_folder.is_dir():\n",
    "                continue\n",
    "                \n",
    "            meta_file = run_folder / 'metadata.json'\n",
    "            summary_file = run_folder / 'summary.json'\n",
    "            csv_file = run_folder / 'training_log.csv'\n",
    "            \n",
    "            if not meta_file.exists():\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                with open(meta_file) as f:\n",
    "                    meta = json.load(f)\n",
    "                \n",
    "                # Load summary if exists\n",
    "                summary = {}\n",
    "                if summary_file.exists():\n",
    "                    with open(summary_file) as f:\n",
    "                        summary = json.load(f)\n",
    "                \n",
    "                # Load training CSV if exists\n",
    "                training_df = None\n",
    "                if csv_file.exists():\n",
    "                    training_df = pd.read_csv(csv_file)\n",
    "                \n",
    "                # Categorize experiment\n",
    "                agent = meta.get('agent_type', 'unknown')\n",
    "                n_step = meta.get('n_step', 1)\n",
    "                per = meta.get('buffer_prioritized', False)\n",
    "                \n",
    "                if agent == 'dqn' and n_step == 1 and not per:\n",
    "                    phase = 'Phase1'\n",
    "                    method = 'DQN'\n",
    "                elif agent == 'deep_sarsa':\n",
    "                    phase = 'Phase1'\n",
    "                    method = 'DeepSARSA'\n",
    "                elif n_step > 1:\n",
    "                    phase = 'Phase2'\n",
    "                    method = f'DQN_n{n_step}'\n",
    "                elif per:\n",
    "                    phase = 'Phase3a'\n",
    "                    method = 'DQN_PER'\n",
    "                elif agent == 'ddqn':\n",
    "                    phase = 'Phase3b'\n",
    "                    method = 'DDQN'\n",
    "                elif 'dueling' in agent:\n",
    "                    phase = 'Phase3c'\n",
    "                    method = 'Dueling_DDQN'\n",
    "                else:\n",
    "                    phase = 'Other'\n",
    "                    method = agent\n",
    "                \n",
    "                results.append({\n",
    "                    'phase': phase,\n",
    "                    'method': method,\n",
    "                    'scenario': meta.get('scenario_short', meta.get('scenario', 'unknown')),\n",
    "                    'seed': meta.get('seed', 0),\n",
    "                    'n_step': n_step,\n",
    "                    'per': per,\n",
    "                    'lr': meta.get('learning_rate', 0.0001),\n",
    "                    'final_reward': summary.get('final_avg_reward', None),\n",
    "                    'best_eval': summary.get('best_eval_reward', None),\n",
    "                    'total_episodes': summary.get('total_episodes', None),\n",
    "                    'training_time': summary.get('training_time_seconds', None),\n",
    "                    'run_dir': str(run_folder),\n",
    "                    'training_df': training_df,\n",
    "                    'date': date_folder.name\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {run_folder}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load all results\n",
    "all_results = load_all_results(RESULTS_DIR)\n",
    "print(f\"\\nLoaded {len(all_results)} experiment runs\")\n",
    "\n",
    "# Convert to DataFrame (without training_df column)\n",
    "df_results = pd.DataFrame([{k:v for k,v in r.items() if k != 'training_df'} for r in all_results])\n",
    "print(f\"\\nColumns: {list(df_results.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Experiment Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary by phase and method\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENT OVERVIEW\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "overview = df_results.groupby(['phase', 'method', 'scenario']).agg({\n",
    "    'seed': 'count',\n",
    "    'final_reward': ['mean', 'std'],\n",
    "    'best_eval': ['mean', 'std']\n",
    "}).round(2)\n",
    "\n",
    "overview.columns = ['runs', 'final_mean', 'final_std', 'best_mean', 'best_std']\n",
    "print(overview.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count by phase\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RUNS PER PHASE\")\n",
    "print(\"=\" * 70)\n",
    "phase_counts = df_results.groupby('phase').size()\n",
    "for phase, count in phase_counts.items():\n",
    "    print(f\"{phase:15} | {count} runs\")\n",
    "print(f\"{'TOTAL':15} | {len(df_results)} runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Phase 1: DQN vs Deep SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1 comparison\n",
    "phase1 = df_results[df_results['phase'] == 'Phase1'].copy()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1: DQN vs Deep SARSA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "phase1_summary = phase1.groupby(['method', 'scenario']).agg({\n",
    "    'final_reward': ['mean', 'std', 'count'],\n",
    "    'best_eval': ['mean', 'std']\n",
    "}).round(2)\n",
    "\n",
    "phase1_summary.columns = ['final_mean', 'final_std', 'n', 'best_mean', 'best_std']\n",
    "print(phase1_summary.to_string())\n",
    "\n",
    "# Statistical comparison\n",
    "print(\"\\n--- Winner by Scenario ---\")\n",
    "for scenario in phase1['scenario'].unique():\n",
    "    dqn = phase1[(phase1['method'] == 'DQN') & (phase1['scenario'] == scenario)]['final_reward']\n",
    "    sarsa = phase1[(phase1['method'] == 'DeepSARSA') & (phase1['scenario'] == scenario)]['final_reward']\n",
    "    \n",
    "    dqn_mean = dqn.mean() if len(dqn) > 0 else float('-inf')\n",
    "    sarsa_mean = sarsa.mean() if len(sarsa) > 0 else float('-inf')\n",
    "    \n",
    "    winner = 'DQN' if dqn_mean > sarsa_mean else 'DeepSARSA'\n",
    "    diff = abs(dqn_mean - sarsa_mean)\n",
    "    print(f\"{scenario:15} | Winner: {winner:12} | Diff: {diff:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1 Learning Curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "scenarios = ['Basic', 'TakeCover', 'Deathmatch']\n",
    "colors = {'DQN': 'blue', 'DeepSARSA': 'orange'}\n",
    "\n",
    "for idx, scenario in enumerate(scenarios):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for method in ['DQN', 'DeepSARSA']:\n",
    "        method_results = [r for r in all_results \n",
    "                         if r['phase'] == 'Phase1' \n",
    "                         and r['method'] == method \n",
    "                         and r['scenario'] == scenario\n",
    "                         and r['training_df'] is not None]\n",
    "        \n",
    "        if not method_results:\n",
    "            continue\n",
    "            \n",
    "        # Average across seeds\n",
    "        all_rewards = []\n",
    "        for r in method_results:\n",
    "            df = r['training_df']\n",
    "            if 'episode' in df.columns and 'reward' in df.columns:\n",
    "                all_rewards.append(df.set_index('episode')['reward'])\n",
    "        \n",
    "        if all_rewards:\n",
    "            combined = pd.concat(all_rewards, axis=1)\n",
    "            mean_reward = combined.mean(axis=1)\n",
    "            std_reward = combined.std(axis=1)\n",
    "            \n",
    "            # Smooth with rolling average\n",
    "            mean_smooth = mean_reward.rolling(50, min_periods=1).mean()\n",
    "            \n",
    "            ax.plot(mean_smooth.index, mean_smooth.values, \n",
    "                   label=method, color=colors[method], linewidth=2)\n",
    "            ax.fill_between(mean_smooth.index, \n",
    "                           mean_smooth.values - std_reward.rolling(50, min_periods=1).mean().values,\n",
    "                           mean_smooth.values + std_reward.rolling(50, min_periods=1).mean().values,\n",
    "                           alpha=0.2, color=colors[method])\n",
    "    \n",
    "    ax.set_title(f'{scenario}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Reward')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Phase 1: DQN vs Deep SARSA Learning Curves', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('phase1_learning_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: phase1_learning_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Phase 2: N-Step Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: N-step comparison\n",
    "# Compare DQN (n=1) from Phase1 with DQN_n3 from Phase2\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 2: N-STEP ABLATION (n=1 vs n=3)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get baseline DQN (n=1)\n",
    "baseline = df_results[(df_results['method'] == 'DQN') & (df_results['phase'] == 'Phase1')].copy()\n",
    "baseline['method'] = 'DQN_n1'\n",
    "\n",
    "# Get n-step results\n",
    "nstep = df_results[df_results['phase'] == 'Phase2'].copy()\n",
    "\n",
    "# Combine for comparison\n",
    "nstep_comparison = pd.concat([baseline, nstep])\n",
    "\n",
    "nstep_summary = nstep_comparison.groupby(['method', 'scenario']).agg({\n",
    "    'final_reward': ['mean', 'std', 'count'],\n",
    "    'best_eval': ['mean', 'std']\n",
    "}).round(2)\n",
    "\n",
    "nstep_summary.columns = ['final_mean', 'final_std', 'n', 'best_mean', 'best_std']\n",
    "print(nstep_summary.to_string())\n",
    "\n",
    "# Winner analysis\n",
    "print(\"\\n--- N-step Impact by Scenario ---\")\n",
    "for scenario in nstep_comparison['scenario'].unique():\n",
    "    n1 = nstep_comparison[(nstep_comparison['method'] == 'DQN_n1') & (nstep_comparison['scenario'] == scenario)]['final_reward'].mean()\n",
    "    n3 = nstep_comparison[(nstep_comparison['method'] == 'DQN_n3') & (nstep_comparison['scenario'] == scenario)]['final_reward'].mean()\n",
    "    \n",
    "    if pd.notna(n1) and pd.notna(n3):\n",
    "        diff = n3 - n1\n",
    "        pct = (diff / abs(n1)) * 100 if n1 != 0 else 0\n",
    "        better = \"n=3 better\" if diff > 0 else \"n=1 better\"\n",
    "        print(f\"{scenario:15} | n=1: {n1:8.2f} | n=3: {n3:8.2f} | Diff: {diff:+8.2f} ({pct:+.1f}%) | {better}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Phase 3: Extensions Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3: All extensions vs baseline DQN\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 3: EXTENSIONS vs BASELINE DQN\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get all Phase 3 results\n",
    "phase3 = df_results[df_results['phase'].str.startswith('Phase3')].copy()\n",
    "\n",
    "# Baseline DQN for comparison\n",
    "baseline_dqn = df_results[(df_results['method'] == 'DQN') & (df_results['phase'] == 'Phase1')].copy()\n",
    "baseline_dqn['method'] = 'DQN_baseline'\n",
    "\n",
    "# Combine\n",
    "extensions_comparison = pd.concat([baseline_dqn, phase3])\n",
    "\n",
    "ext_summary = extensions_comparison.groupby(['method', 'scenario']).agg({\n",
    "    'final_reward': ['mean', 'std', 'count'],\n",
    "    'best_eval': ['mean', 'std']\n",
    "}).round(2)\n",
    "\n",
    "ext_summary.columns = ['final_mean', 'final_std', 'n', 'best_mean', 'best_std']\n",
    "print(ext_summary.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "scenarios = ['Basic', 'TakeCover', 'Deathmatch']\n",
    "methods_order = ['DQN_baseline', 'DQN_n3', 'DQN_PER', 'DDQN', 'Dueling_DDQN']\n",
    "colors = ['gray', 'blue', 'green', 'orange', 'red']\n",
    "\n",
    "for idx, scenario in enumerate(scenarios):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Combine all methods for this scenario\n",
    "    all_methods = pd.concat([baseline_dqn, nstep, phase3])\n",
    "    scenario_data = all_methods[all_methods['scenario'] == scenario]\n",
    "    \n",
    "    means = []\n",
    "    stds = []\n",
    "    labels = []\n",
    "    \n",
    "    for method in methods_order:\n",
    "        method_data = scenario_data[scenario_data['method'] == method]['final_reward']\n",
    "        if len(method_data) > 0:\n",
    "            means.append(method_data.mean())\n",
    "            stds.append(method_data.std() if len(method_data) > 1 else 0)\n",
    "            labels.append(method.replace('DQN_', '').replace('_', '\\n'))\n",
    "    \n",
    "    if means:\n",
    "        x = range(len(means))\n",
    "        bars = ax.bar(x, means, yerr=stds, capsize=5, color=colors[:len(means)], alpha=0.7)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels, fontsize=9)\n",
    "        ax.set_ylabel('Final Reward')\n",
    "        ax.set_title(f'{scenario}', fontsize=12, fontweight='bold')\n",
    "        ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('All Methods Comparison by Scenario', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('all_methods_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: all_methods_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. IEEE Report Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LaTeX tables for IEEE report\n",
    "print(\"=\" * 70)\n",
    "print(\"LATEX TABLES FOR IEEE REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Table 1: Phase 1 Results\n",
    "print(\"\\n% Table 1: DQN vs Deep SARSA\")\n",
    "print(\"\\\\begin{table}[h]\")\n",
    "print(\"\\\\centering\")\n",
    "print(\"\\\\caption{Phase 1: DQN vs Deep SARSA Comparison}\")\n",
    "print(\"\\\\begin{tabular}{|l|l|r|r|}\")\n",
    "print(\"\\\\hline\")\n",
    "print(\"Scenario & Method & Final Reward & Best Eval \\\\\\\\\")\n",
    "print(\"\\\\hline\")\n",
    "\n",
    "for scenario in ['Basic', 'TakeCover', 'Deathmatch']:\n",
    "    for method in ['DQN', 'DeepSARSA']:\n",
    "        data = phase1[(phase1['scenario'] == scenario) & (phase1['method'] == method)]\n",
    "        if len(data) > 0:\n",
    "            final = f\"{data['final_reward'].mean():.1f} $\\\\pm$ {data['final_reward'].std():.1f}\"\n",
    "            best = f\"{data['best_eval'].mean():.1f} $\\\\pm$ {data['best_eval'].std():.1f}\"\n",
    "            print(f\"{scenario} & {method} & {final} & {best} \\\\\\\\\")\n",
    "    print(\"\\\\hline\")\n",
    "\n",
    "print(\"\\\\end{tabular}\")\n",
    "print(\"\\\\end{table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 2: All Extensions Comparison\n",
    "print(\"\\n% Table 2: Extensions Comparison\")\n",
    "print(\"\\\\begin{table}[h]\")\n",
    "print(\"\\\\centering\")\n",
    "print(\"\\\\caption{DQN Extensions Comparison (Final Reward)}\")\n",
    "print(\"\\\\begin{tabular}{|l|r|r|r|}\")\n",
    "print(\"\\\\hline\")\n",
    "print(\"Method & Basic & TakeCover & Deathmatch \\\\\\\\\")\n",
    "print(\"\\\\hline\")\n",
    "\n",
    "all_methods_df = pd.concat([baseline_dqn, nstep, phase3])\n",
    "\n",
    "for method in ['DQN_baseline', 'DQN_n3', 'DQN_PER', 'DDQN', 'Dueling_DDQN']:\n",
    "    row = [method.replace('_', ' ')]\n",
    "    for scenario in ['Basic', 'TakeCover', 'Deathmatch']:\n",
    "        data = all_methods_df[(all_methods_df['method'] == method) & (all_methods_df['scenario'] == scenario)]\n",
    "        if len(data) > 0:\n",
    "            val = f\"{data['final_reward'].mean():.1f}\"\n",
    "        else:\n",
    "            val = \"-\"\n",
    "        row.append(val)\n",
    "    print(f\"{row[0]} & {row[1]} & {row[2]} & {row[3]} \\\\\\\\\")\n",
    "\n",
    "print(\"\\\\hline\")\n",
    "print(\"\\\\end{tabular}\")\n",
    "print(\"\\\\end{table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nTotal Experiments: {len(df_results)}\")\n",
    "print(f\"Date Range: {df_results['date'].min()} to {df_results['date'].max()}\")\n",
    "\n",
    "# Best method per scenario\n",
    "print(\"\\n--- Best Method per Scenario (by Final Reward) ---\")\n",
    "all_methods_df = pd.concat([baseline_dqn, nstep, phase3])\n",
    "\n",
    "for scenario in ['Basic', 'TakeCover', 'Deathmatch']:\n",
    "    scenario_data = all_methods_df[all_methods_df['scenario'] == scenario]\n",
    "    if len(scenario_data) > 0:\n",
    "        best_method = scenario_data.groupby('method')['final_reward'].mean().idxmax()\n",
    "        best_reward = scenario_data.groupby('method')['final_reward'].mean().max()\n",
    "        print(f\"{scenario:15} | Best: {best_method:20} | Reward: {best_reward:.2f}\")\n",
    "\n",
    "# Training time\n",
    "print(\"\\n--- Total Training Time ---\")\n",
    "total_time = df_results['training_time'].sum()\n",
    "if pd.notna(total_time):\n",
    "    hours = total_time / 3600\n",
    "    print(f\"Total: {hours:.1f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export full results to CSV\n",
    "export_df = df_results.drop(columns=['run_dir'], errors='ignore')\n",
    "export_df.to_csv('experiment_results_summary.csv', index=False)\n",
    "print(\"Exported: experiment_results_summary.csv\")\n",
    "\n",
    "# Show the export\n",
    "print(\"\\nFull Results Table:\")\n",
    "display(export_df.sort_values(['phase', 'method', 'scenario', 'seed']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Key Findings for Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"KEY FINDINGS FOR IEEE REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "1. PHASE 1: DQN vs Deep SARSA\n",
    "   - [Fill based on results above]\n",
    "   - Off-policy (DQN) vs On-policy (SARSA) comparison\n",
    "   \n",
    "2. PHASE 2: N-Step Returns\n",
    "   - n=3 vs n=1 (baseline) comparison\n",
    "   - TD/MC tradeoff: bias vs variance\n",
    "   - [Fill: which scenarios benefited?]\n",
    "   \n",
    "3. PHASE 3: Extensions\n",
    "   - PER: Priority sampling impact\n",
    "   - DDQN: Overestimation bias reduction\n",
    "   - Dueling: V/A separation benefit\n",
    "   - [Fill: Rainbow ablation alignment?]\n",
    "\n",
    "4. SCENARIO INSIGHTS\n",
    "   - Basic: [short episodes, simple task]\n",
    "   - TakeCover: [survival, longer episodes]\n",
    "   - Deathmatch: [combat, complex rewards]\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
