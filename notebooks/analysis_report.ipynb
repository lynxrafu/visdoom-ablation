{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ViZDoom Ablation Study - Results Analysis\n\n**IEEE Report Data Generation**\n\nAnalyzes all 34 experiment runs and generates tables/plots for the report.\n\n**Sections:**\n1. Setup & Load Data\n2. Experiment Overview\n3. Phase 1: DQN vs Deep SARSA\n4. Phase 2: N-Step Ablation\n5. Phase 3: Extensions (PER, DDQN, Dueling)\n6. Statistical Significance Tests\n7. IEEE LaTeX Tables\n8. Key Findings Summary\n9. Export to Drive"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Check if on Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    RESULTS_DIR = Path('/content/drive/MyDrive/vizdoom-ablation-results')\n",
    "else:\n",
    "    RESULTS_DIR = Path('results')\n",
    "\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"Exists: {RESULTS_DIR.exists()}\")\n",
    "\n",
    "# List date folders\n",
    "if RESULTS_DIR.exists():\n",
    "    date_folders = [f.name for f in RESULTS_DIR.iterdir() if f.is_dir() and not f.name.startswith('.')]\n",
    "    print(f\"Date folders: {sorted(date_folders)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_results(results_dir):\n",
    "    \"\"\"Load all experiment metadata and summaries.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for date_folder in sorted(results_dir.iterdir()):\n",
    "        if not date_folder.is_dir() or date_folder.name.startswith('.'):\n",
    "            continue\n",
    "        if not date_folder.name.startswith('202'):  # Skip non-date folders\n",
    "            continue\n",
    "            \n",
    "        for run_folder in sorted(date_folder.iterdir()):\n",
    "            if not run_folder.is_dir():\n",
    "                continue\n",
    "                \n",
    "            meta_file = run_folder / 'metadata.json'\n",
    "            summary_file = run_folder / 'summary.json'\n",
    "            csv_file = run_folder / 'training_log.csv'\n",
    "            \n",
    "            if not meta_file.exists():\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                with open(meta_file) as f:\n",
    "                    meta = json.load(f)\n",
    "                \n",
    "                # Load summary if exists\n",
    "                summary = {}\n",
    "                if summary_file.exists():\n",
    "                    with open(summary_file) as f:\n",
    "                        summary = json.load(f)\n",
    "                \n",
    "                # Load training CSV if exists\n",
    "                training_df = None\n",
    "                if csv_file.exists():\n",
    "                    training_df = pd.read_csv(csv_file)\n",
    "                \n",
    "                # Categorize experiment\n",
    "                agent = meta.get('agent_type', 'unknown')\n",
    "                n_step = meta.get('n_step', 1)\n",
    "                per = meta.get('buffer_prioritized', False)\n",
    "                \n",
    "                if agent == 'dqn' and n_step == 1 and not per:\n",
    "                    phase = 'Phase1'\n",
    "                    method = 'DQN'\n",
    "                elif agent == 'deep_sarsa':\n",
    "                    phase = 'Phase1'\n",
    "                    method = 'DeepSARSA'\n",
    "                elif n_step > 1:\n",
    "                    phase = 'Phase2'\n",
    "                    method = f'DQN_n{n_step}'\n",
    "                elif per:\n",
    "                    phase = 'Phase3a'\n",
    "                    method = 'DQN_PER'\n",
    "                elif agent == 'ddqn':\n",
    "                    phase = 'Phase3b'\n",
    "                    method = 'DDQN'\n",
    "                elif 'dueling' in agent:\n",
    "                    phase = 'Phase3c'\n",
    "                    method = 'Dueling_DDQN'\n",
    "                else:\n",
    "                    phase = 'Other'\n",
    "                    method = agent\n",
    "                \n",
    "                results.append({\n",
    "                    'phase': phase,\n",
    "                    'method': method,\n",
    "                    'scenario': meta.get('scenario_short', meta.get('scenario', 'unknown')),\n",
    "                    'seed': meta.get('seed', 0),\n",
    "                    'n_step': n_step,\n",
    "                    'per': per,\n",
    "                    'lr': meta.get('learning_rate', 0.0001),\n",
    "                    'gamma': meta.get('gamma', 0.99),\n",
    "                    # Correct field names from summary.json\n",
    "                    'final_reward': summary.get('final_avg_reward_100', None),\n",
    "                    'best_eval': summary.get('best_eval_reward', None),\n",
    "                    'total_episodes': summary.get('num_episodes', meta.get('num_episodes', None)),\n",
    "                    'training_hours': summary.get('total_time_hours', None),\n",
    "                    'run_dir': str(run_folder),\n",
    "                    'run_name': meta.get('run_name', run_folder.name),\n",
    "                    'training_df': training_df,\n",
    "                    'date': date_folder.name\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {run_folder.name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load all results\n",
    "all_results = load_all_results(RESULTS_DIR)\n",
    "print(f\"\\nLoaded {len(all_results)} experiment runs\")\n",
    "\n",
    "# Convert to DataFrame (without training_df column)\n",
    "df = pd.DataFrame([{k:v for k,v in r.items() if k != 'training_df'} for r in all_results])\n",
    "\n",
    "if len(df) > 0:\n",
    "    print(f\"\\nPhases: {df['phase'].unique()}\")\n",
    "    print(f\"Methods: {df['method'].unique()}\")\n",
    "    print(f\"Scenarios: {df['scenario'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Experiment Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full results table\n",
    "print(\"=\" * 80)\n",
    "print(\"ALL EXPERIMENTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "display_cols = ['phase', 'method', 'scenario', 'seed', 'final_reward', 'best_eval', 'training_hours']\n",
    "print(df[display_cols].sort_values(['phase', 'method', 'scenario', 'seed']).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY BY METHOD AND SCENARIO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary = df.groupby(['phase', 'method', 'scenario']).agg({\n",
    "    'seed': 'count',\n",
    "    'final_reward': ['mean', 'std'],\n",
    "    'best_eval': ['mean', 'std'],\n",
    "    'training_hours': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "summary.columns = ['n_runs', 'final_mean', 'final_std', 'best_mean', 'best_std', 'hours']\n",
    "print(summary.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs per phase\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RUNS PER PHASE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "phase_summary = df.groupby('phase').agg({\n",
    "    'seed': 'count',\n",
    "    'training_hours': 'sum'\n",
    "}).round(2)\n",
    "phase_summary.columns = ['runs', 'total_hours']\n",
    "\n",
    "for phase, row in phase_summary.iterrows():\n",
    "    print(f\"{phase:10} | {int(row['runs']):2} runs | {row['total_hours']:.1f} hours\")\n",
    "\n",
    "print(f\"{'TOTAL':10} | {len(df):2} runs | {df['training_hours'].sum():.1f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Phase 1: DQN vs Deep SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1 detailed comparison\n",
    "phase1 = df[df['phase'] == 'Phase1'].copy()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 1: DQN vs Deep SARSA (Off-policy vs On-policy)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Pivot table\n",
    "phase1_pivot = phase1.pivot_table(\n",
    "    values=['final_reward', 'best_eval'],\n",
    "    index='scenario',\n",
    "    columns='method',\n",
    "    aggfunc=['mean', 'std']\n",
    ").round(2)\n",
    "\n",
    "print(phase1_pivot.to_string())\n",
    "\n",
    "# Winner per scenario\n",
    "print(\"\\n--- Winner by Scenario ---\")\n",
    "for scenario in ['Basic', 'TakeCover', 'Deathmatch']:\n",
    "    dqn_data = phase1[(phase1['method'] == 'DQN') & (phase1['scenario'] == scenario)]\n",
    "    sarsa_data = phase1[(phase1['method'] == 'DeepSARSA') & (phase1['scenario'] == scenario)]\n",
    "    \n",
    "    dqn_best = dqn_data['best_eval'].mean() if len(dqn_data) > 0 else float('-inf')\n",
    "    sarsa_best = sarsa_data['best_eval'].mean() if len(sarsa_data) > 0 else float('-inf')\n",
    "    \n",
    "    winner = 'DQN' if dqn_best > sarsa_best else 'DeepSARSA'\n",
    "    diff = dqn_best - sarsa_best\n",
    "    print(f\"{scenario:12} | DQN: {dqn_best:8.2f} | SARSA: {sarsa_best:8.2f} | Winner: {winner} ({diff:+.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1 Learning Curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "scenarios = ['Basic', 'TakeCover', 'Deathmatch']\n",
    "colors = {'DQN': '#1f77b4', 'DeepSARSA': '#ff7f0e'}\n",
    "\n",
    "for idx, scenario in enumerate(scenarios):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for method in ['DQN', 'DeepSARSA']:\n",
    "        method_results = [r for r in all_results \n",
    "                         if r['phase'] == 'Phase1' \n",
    "                         and r['method'] == method \n",
    "                         and r['scenario'] == scenario\n",
    "                         and r['training_df'] is not None]\n",
    "        \n",
    "        if not method_results:\n",
    "            continue\n",
    "            \n",
    "        # Collect all rewards\n",
    "        all_rewards = []\n",
    "        for r in method_results:\n",
    "            tdf = r['training_df']\n",
    "            if 'episode' in tdf.columns and 'reward' in tdf.columns:\n",
    "                rewards = tdf.set_index('episode')['reward']\n",
    "                all_rewards.append(rewards)\n",
    "        \n",
    "        if all_rewards:\n",
    "            combined = pd.concat(all_rewards, axis=1)\n",
    "            mean_reward = combined.mean(axis=1)\n",
    "            std_reward = combined.std(axis=1)\n",
    "            \n",
    "            # Rolling average for smoothing\n",
    "            window = 50\n",
    "            mean_smooth = mean_reward.rolling(window, min_periods=1).mean()\n",
    "            std_smooth = std_reward.rolling(window, min_periods=1).mean()\n",
    "            \n",
    "            ax.plot(mean_smooth.index, mean_smooth.values, \n",
    "                   label=method, color=colors[method], linewidth=1.5)\n",
    "            ax.fill_between(mean_smooth.index, \n",
    "                           mean_smooth.values - std_smooth.values,\n",
    "                           mean_smooth.values + std_smooth.values,\n",
    "                           alpha=0.2, color=colors[method])\n",
    "    \n",
    "    ax.set_title(f'{scenario}', fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Reward')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Phase 1: DQN vs Deep SARSA Learning Curves (3 seeds avg)', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('phase1_learning_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: phase1_learning_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Phase 2: N-Step Ablation (n=1 vs n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Compare DQN n=1 (baseline) vs DQN n=3\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 2: N-STEP ABLATION (TD/MC Bridge)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Baseline: DQN with n=1 from Phase 1\n",
    "baseline_n1 = df[(df['method'] == 'DQN') & (df['phase'] == 'Phase1')].copy()\n",
    "baseline_n1['method'] = 'DQN_n1'\n",
    "\n",
    "# N-step: DQN with n=3 from Phase 2\n",
    "nstep_n3 = df[df['phase'] == 'Phase2'].copy()\n",
    "\n",
    "# Combine\n",
    "nstep_compare = pd.concat([baseline_n1, nstep_n3])\n",
    "\n",
    "# Pivot\n",
    "nstep_pivot = nstep_compare.pivot_table(\n",
    "    values='best_eval',\n",
    "    index='scenario',\n",
    "    columns='method',\n",
    "    aggfunc=['mean', 'std', 'count']\n",
    ").round(2)\n",
    "\n",
    "print(nstep_pivot.to_string())\n",
    "\n",
    "# Impact analysis\n",
    "print(\"\\n--- N-step Impact (n=3 vs n=1) ---\")\n",
    "for scenario in ['Basic', 'TakeCover', 'Deathmatch']:\n",
    "    n1 = nstep_compare[(nstep_compare['method'] == 'DQN_n1') & (nstep_compare['scenario'] == scenario)]['best_eval'].mean()\n",
    "    n3 = nstep_compare[(nstep_compare['method'] == 'DQN_n3') & (nstep_compare['scenario'] == scenario)]['best_eval'].mean()\n",
    "    \n",
    "    if pd.notna(n1) and pd.notna(n3):\n",
    "        diff = n3 - n1\n",
    "        pct = (diff / abs(n1)) * 100 if n1 != 0 else 0\n",
    "        verdict = \"BETTER\" if diff > 0 else \"WORSE\"\n",
    "        print(f\"{scenario:12} | n=1: {n1:8.2f} | n=3: {n3:8.2f} | Diff: {diff:+8.2f} ({pct:+6.1f}%) {verdict}\")\n",
    "    else:\n",
    "        print(f\"{scenario:12} | Data missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Phase 3: Extensions (PER, DDQN, Dueling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3: All extensions vs baseline\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 3: DQN EXTENSIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Baseline DQN\n",
    "baseline = df[(df['method'] == 'DQN') & (df['phase'] == 'Phase1')].copy()\n",
    "baseline['method'] = 'Baseline_DQN'\n",
    "\n",
    "# All Phase 3\n",
    "phase3 = df[df['phase'].str.startswith('Phase3')].copy()\n",
    "\n",
    "# Combine\n",
    "ext_compare = pd.concat([baseline, phase3])\n",
    "\n",
    "# Pivot for best_eval\n",
    "ext_pivot = ext_compare.pivot_table(\n",
    "    values='best_eval',\n",
    "    index='scenario',\n",
    "    columns='method',\n",
    "    aggfunc='mean'\n",
    ").round(2)\n",
    "\n",
    "print(\"Mean Best Eval by Method:\")\n",
    "print(ext_pivot.to_string())\n",
    "\n",
    "# Improvement over baseline\n",
    "print(\"\\n--- Improvement over Baseline DQN ---\")\n",
    "for scenario in ['Basic', 'TakeCover']:\n",
    "    print(f\"\\n{scenario}:\")\n",
    "    base_val = ext_pivot.loc[scenario, 'Baseline_DQN'] if scenario in ext_pivot.index else None\n",
    "    \n",
    "    if base_val is None:\n",
    "        continue\n",
    "        \n",
    "    for method in ['DQN_PER', 'DDQN', 'Dueling_DDQN']:\n",
    "        if method in ext_pivot.columns and scenario in ext_pivot.index:\n",
    "            val = ext_pivot.loc[scenario, method]\n",
    "            if pd.notna(val):\n",
    "                diff = val - base_val\n",
    "                pct = (diff / abs(base_val)) * 100 if base_val != 0 else 0\n",
    "                print(f\"  {method:15} | {val:8.2f} | vs baseline: {diff:+8.2f} ({pct:+6.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart: All methods comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "scenarios = ['Basic', 'TakeCover']\n",
    "method_order = ['Baseline_DQN', 'DQN_n3', 'DQN_PER', 'DDQN', 'Dueling_DDQN']\n",
    "method_labels = ['DQN\\n(baseline)', 'DQN\\nn=3', 'DQN\\n+PER', 'DDQN', 'Dueling\\n+DDQN']\n",
    "colors = ['#7f7f7f', '#1f77b4', '#2ca02c', '#ff7f0e', '#d62728']\n",
    "\n",
    "# Combine all data\n",
    "all_compare = pd.concat([baseline, nstep_n3, phase3])\n",
    "\n",
    "for idx, scenario in enumerate(scenarios):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    means = []\n",
    "    stds = []\n",
    "    valid_labels = []\n",
    "    valid_colors = []\n",
    "    \n",
    "    for i, method in enumerate(method_order):\n",
    "        data = all_compare[(all_compare['method'] == method) & (all_compare['scenario'] == scenario)]['best_eval']\n",
    "        if len(data) > 0:\n",
    "            means.append(data.mean())\n",
    "            stds.append(data.std() if len(data) > 1 else 0)\n",
    "            valid_labels.append(method_labels[i])\n",
    "            valid_colors.append(colors[i])\n",
    "    \n",
    "    if means:\n",
    "        x = range(len(means))\n",
    "        bars = ax.bar(x, means, yerr=stds, capsize=4, color=valid_colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(valid_labels, fontsize=9)\n",
    "        ax.set_ylabel('Best Eval Reward')\n",
    "        ax.set_title(f'{scenario}', fontsize=11, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, mean in zip(bars, means):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, \n",
    "                   f'{mean:.0f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.suptitle('All Methods Comparison (Best Eval Reward)', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('all_methods_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: all_methods_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 5b. Phase 2 & 3 Learning Curves",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Phase 2: N-step Learning Curves\nfig, axes = plt.subplots(1, 3, figsize=(14, 4))\n\nscenarios = ['Basic', 'TakeCover', 'Deathmatch']\ncolors = {'DQN': '#1f77b4', 'DQN_n3': '#ff7f0e'}\nlabels = {'DQN': 'DQN (n=1)', 'DQN_n3': 'DQN (n=3)'}\n\nfor idx, scenario in enumerate(scenarios):\n    ax = axes[idx]\n\n    # n=1 from Phase 1\n    for method, phase_filter in [('DQN', 'Phase1'), ('DQN_n3', 'Phase2')]:\n        method_results = [r for r in all_results\n                         if r['phase'] == phase_filter\n                         and (r['method'] == method or (method == 'DQN_n3' and r['method'].startswith('DQN_n')))\n                         and r['scenario'] == scenario\n                         and r['training_df'] is not None]\n\n        if not method_results:\n            continue\n\n        all_rewards = []\n        for r in method_results:\n            tdf = r['training_df']\n            if 'episode' in tdf.columns and 'reward' in tdf.columns:\n                rewards = tdf.set_index('episode')['reward']\n                all_rewards.append(rewards)\n\n        if all_rewards:\n            combined = pd.concat(all_rewards, axis=1)\n            mean_reward = combined.mean(axis=1)\n            std_reward = combined.std(axis=1)\n\n            window = 50\n            mean_smooth = mean_reward.rolling(window, min_periods=1).mean()\n            std_smooth = std_reward.rolling(window, min_periods=1).mean()\n\n            ax.plot(mean_smooth.index, mean_smooth.values,\n                   label=labels[method], color=colors[method], linewidth=1.5)\n            ax.fill_between(mean_smooth.index,\n                           mean_smooth.values - std_smooth.values,\n                           mean_smooth.values + std_smooth.values,\n                           alpha=0.2, color=colors[method])\n\n    ax.set_title(f'{scenario}', fontsize=11, fontweight='bold')\n    ax.set_xlabel('Episode')\n    ax.set_ylabel('Reward')\n    ax.legend(loc='lower right')\n    ax.grid(True, alpha=0.3)\n\nplt.suptitle('Phase 2: N-Step Learning Curves (n=1 vs n=3)', fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.savefig('phase2_learning_curves.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(\"Saved: phase2_learning_curves.png\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Phase 3: Extensions Learning Curves (Basic and TakeCover)\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nscenarios = ['Basic', 'TakeCover']\nmethod_colors = {\n    'DQN': '#7f7f7f',\n    'DQN_PER': '#2ca02c',\n    'DDQN': '#ff7f0e',\n    'Dueling_DDQN': '#d62728'\n}\nmethod_labels = {\n    'DQN': 'DQN (baseline)',\n    'DQN_PER': 'DQN + PER',\n    'DDQN': 'DDQN',\n    'Dueling_DDQN': 'Dueling + DDQN'\n}\n\nfor idx, scenario in enumerate(scenarios):\n    ax = axes[idx]\n\n    for method in ['DQN', 'DQN_PER', 'DDQN', 'Dueling_DDQN']:\n        if method == 'DQN':\n            method_results = [r for r in all_results\n                             if r['phase'] == 'Phase1'\n                             and r['method'] == 'DQN'\n                             and r['scenario'] == scenario\n                             and r['training_df'] is not None]\n        else:\n            method_results = [r for r in all_results\n                             if r['phase'].startswith('Phase3')\n                             and r['method'] == method\n                             and r['scenario'] == scenario\n                             and r['training_df'] is not None]\n\n        if not method_results:\n            continue\n\n        all_rewards = []\n        for r in method_results:\n            tdf = r['training_df']\n            if 'episode' in tdf.columns and 'reward' in tdf.columns:\n                rewards = tdf.set_index('episode')['reward']\n                all_rewards.append(rewards)\n\n        if all_rewards:\n            combined = pd.concat(all_rewards, axis=1)\n            mean_reward = combined.mean(axis=1)\n\n            window = 50\n            mean_smooth = mean_reward.rolling(window, min_periods=1).mean()\n\n            ax.plot(mean_smooth.index, mean_smooth.values,\n                   label=method_labels[method], color=method_colors[method], linewidth=1.5)\n\n    ax.set_title(f'{scenario}', fontsize=11, fontweight='bold')\n    ax.set_xlabel('Episode')\n    ax.set_ylabel('Reward')\n    ax.legend(loc='lower right', fontsize=8)\n    ax.grid(True, alpha=0.3)\n\nplt.suptitle('Phase 3: DQN Extensions Learning Curves', fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.savefig('phase3_learning_curves.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(\"Saved: phase3_learning_curves.png\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 5c. Statistical Significance Tests (t-test)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Statistical Significance Tests (Independent t-tests)\nfrom scipy import stats\n\nprint(\"=\" * 80)\nprint(\"STATISTICAL SIGNIFICANCE (Independent t-tests)\")\nprint(\"=\" * 80)\n\ndef ttest_comparison(group1, group2):\n    \"\"\"Perform t-test and return results.\"\"\"\n    if len(group1) < 2 or len(group2) < 2:\n        return None, None, \"n/a\"\n    t_stat, p_value = stats.ttest_ind(group1, group2)\n    sig = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n    return t_stat, p_value, sig\n\n# Phase 1: DQN vs Deep SARSA\nprint(\"\\n--- Phase 1: DQN vs Deep SARSA ---\")\nprint(f\"{'Scenario':<12} | {'t-stat':>8} | {'p-value':>8} | {'Sig':>4}\")\nprint(\"-\" * 45)\nfor scenario in ['Basic', 'TakeCover', 'Deathmatch']:\n    dqn = phase1[(phase1['method'] == 'DQN') & (phase1['scenario'] == scenario)]['best_eval'].dropna()\n    sarsa = phase1[(phase1['method'] == 'DeepSARSA') & (phase1['scenario'] == scenario)]['best_eval'].dropna()\n    \n    if len(dqn) > 0 and len(sarsa) > 0:\n        t_stat, p_val, sig = ttest_comparison(dqn.values, sarsa.values)\n        if t_stat is not None:\n            print(f\"{scenario:<12} | {t_stat:>+8.3f} | {p_val:>8.4f} | {sig:>4}\")\n        else:\n            print(f\"{scenario:<12} | {'n/a':>8} | {'n/a':>8} | {sig:>4}\")\n\n# Phase 2: n=1 vs n=3\nprint(\"\\n--- Phase 2: N-step (n=1 vs n=3) ---\")\nprint(f\"{'Scenario':<12} | {'t-stat':>8} | {'p-value':>8} | {'Sig':>4}\")\nprint(\"-\" * 45)\nfor scenario in ['Basic', 'TakeCover', 'Deathmatch']:\n    n1 = nstep_compare[(nstep_compare['method'] == 'DQN_n1') & (nstep_compare['scenario'] == scenario)]['best_eval'].dropna()\n    n3 = nstep_compare[(nstep_compare['method'] == 'DQN_n3') & (nstep_compare['scenario'] == scenario)]['best_eval'].dropna()\n    \n    if len(n1) > 0 and len(n3) > 0:\n        t_stat, p_val, sig = ttest_comparison(n1.values, n3.values)\n        if t_stat is not None:\n            print(f\"{scenario:<12} | {t_stat:>+8.3f} | {p_val:>8.4f} | {sig:>4}\")\n        else:\n            print(f\"{scenario:<12} | {'n/a':>8} | {'n/a':>8} | {sig:>4}\")\n\n# Phase 3: Extensions vs Baseline\nprint(\"\\n--- Phase 3: Extensions vs Baseline DQN ---\")\nprint(f\"{'Scenario':<10} {'Method':<15} | {'t-stat':>8} | {'p-value':>8} | {'Sig':>4}\")\nprint(\"-\" * 60)\nfor scenario in ['Basic', 'TakeCover']:\n    baseline_data = all_compare[(all_compare['method'] == 'Baseline_DQN') & (all_compare['scenario'] == scenario)]['best_eval'].dropna()\n    \n    for method in ['DQN_PER', 'DDQN', 'Dueling_DDQN']:\n        ext_data = all_compare[(all_compare['method'] == method) & (all_compare['scenario'] == scenario)]['best_eval'].dropna()\n        \n        if len(baseline_data) > 0 and len(ext_data) > 0:\n            t_stat, p_val, sig = ttest_comparison(baseline_data.values, ext_data.values)\n            if t_stat is not None:\n                print(f\"{scenario:<10} {method:<15} | {t_stat:>+8.3f} | {p_val:>8.4f} | {sig:>4}\")\n            else:\n                print(f\"{scenario:<10} {method:<15} | {'n/a':>8} | {'n/a':>8} | {sig:>4}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Significance levels: *** p<0.001, ** p<0.01, * p<0.05, ns=not significant\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. IEEE Report Tables (LaTeX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LATEX TABLES FOR IEEE REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Table 1: Phase 1\n",
    "print(\"\\n% ===== TABLE 1: DQN vs Deep SARSA =====\")\n",
    "print(r\"\\begin{table}[h]\")\n",
    "print(r\"\\centering\")\n",
    "print(r\"\\caption{Phase 1: DQN vs Deep SARSA Comparison}\")\n",
    "print(r\"\\label{tab:phase1}\")\n",
    "print(r\"\\begin{tabular}{|l|l|c|c|}\")\n",
    "print(r\"\\hline\")\n",
    "print(r\"\\textbf{Scenario} & \\textbf{Method} & \\textbf{Best Eval} & \\textbf{Final Reward} \\\\\")\n",
    "print(r\"\\hline\")\n",
    "\n",
    "for scenario in ['Basic', 'TakeCover', 'Deathmatch']:\n",
    "    for method in ['DQN', 'DeepSARSA']:\n",
    "        data = phase1[(phase1['scenario'] == scenario) & (phase1['method'] == method)]\n",
    "        if len(data) > 0:\n",
    "            best_mean = data['best_eval'].mean()\n",
    "            best_std = data['best_eval'].std()\n",
    "            final_mean = data['final_reward'].mean() if data['final_reward'].notna().any() else float('nan')\n",
    "            final_std = data['final_reward'].std() if data['final_reward'].notna().any() else float('nan')\n",
    "            \n",
    "            best_str = f\"${best_mean:.1f} \\\\pm {best_std:.1f}$\" if pd.notna(best_std) else f\"${best_mean:.1f}$\"\n",
    "            final_str = f\"${final_mean:.1f} \\\\pm {final_std:.1f}$\" if pd.notna(final_std) else \"-\"\n",
    "            \n",
    "            print(f\"{scenario} & {method} & {best_str} & {final_str} \\\\\\\\\")\n",
    "    print(r\"\\hline\")\n",
    "\n",
    "print(r\"\\end{tabular}\")\n",
    "print(r\"\\end{table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 2: All Extensions\n",
    "print(\"\\n% ===== TABLE 2: All Extensions Comparison =====\")\n",
    "print(r\"\\begin{table}[h]\")\n",
    "print(r\"\\centering\")\n",
    "print(r\"\\caption{DQN Extensions Comparison (Best Eval Reward)}\")\n",
    "print(r\"\\label{tab:extensions}\")\n",
    "print(r\"\\begin{tabular}{|l|c|c|c|}\")\n",
    "print(r\"\\hline\")\n",
    "print(r\"\\textbf{Method} & \\textbf{Basic} & \\textbf{TakeCover} & \\textbf{Deathmatch} \\\\\")\n",
    "print(r\"\\hline\")\n",
    "\n",
    "all_compare = pd.concat([baseline, nstep_n3, phase3])\n",
    "method_names = {\n",
    "    'Baseline_DQN': 'DQN (baseline)',\n",
    "    'DQN_n3': 'DQN + n-step (n=3)',\n",
    "    'DQN_PER': 'DQN + PER',\n",
    "    'DDQN': 'Double DQN',\n",
    "    'Dueling_DDQN': 'Dueling + DDQN'\n",
    "}\n",
    "\n",
    "for method in ['Baseline_DQN', 'DQN_n3', 'DQN_PER', 'DDQN', 'Dueling_DDQN']:\n",
    "    row = [method_names.get(method, method)]\n",
    "    for scenario in ['Basic', 'TakeCover', 'Deathmatch']:\n",
    "        data = all_compare[(all_compare['method'] == method) & (all_compare['scenario'] == scenario)]['best_eval']\n",
    "        if len(data) > 0:\n",
    "            mean = data.mean()\n",
    "            std = data.std() if len(data) > 1 else 0\n",
    "            row.append(f\"${mean:.1f} \\\\pm {std:.1f}$\" if std > 0 else f\"${mean:.1f}$\")\n",
    "        else:\n",
    "            row.append(\"-\")\n",
    "    print(f\"{row[0]} & {row[1]} & {row[2]} & {row[3]} \\\\\\\\\")\n",
    "\n",
    "print(r\"\\hline\")\n",
    "print(r\"\\end{tabular}\")\n",
    "print(r\"\\end{table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "export_cols = ['phase', 'method', 'scenario', 'seed', 'n_step', 'per', \n",
    "               'final_reward', 'best_eval', 'training_hours', 'date', 'run_name']\n",
    "export_df = df[export_cols].sort_values(['phase', 'method', 'scenario', 'seed'])\n",
    "\n",
    "export_df.to_csv('all_results.csv', index=False)\n",
    "print(\"Exported: all_results.csv\")\n",
    "\n",
    "# Display\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPLETE RESULTS TABLE\")\n",
    "print(\"=\" * 80)\n",
    "display(export_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 8. Key Findings Summary"
  },
  {
   "cell_type": "code",
   "source": "print(\"=\" * 80)\nprint(\"KEY FINDINGS FOR IEEE REPORT\")\nprint(\"=\" * 80)\n\n# Calculate key metrics\nall_compare = pd.concat([baseline, nstep_n3, phase3])\n\nprint(\"\\n1. PHASE 1: DQN vs Deep SARSA\")\nprint(\"-\" * 40)\nfor scenario in ['Basic', 'TakeCover', 'Deathmatch']:\n    dqn = phase1[(phase1['method'] == 'DQN') & (phase1['scenario'] == scenario)]['best_eval'].mean()\n    sarsa = phase1[(phase1['method'] == 'DeepSARSA') & (phase1['scenario'] == scenario)]['best_eval'].mean()\n    winner = 'DQN' if dqn > sarsa else 'DeepSARSA'\n    print(f\"   {scenario}: {winner} wins (DQN={dqn:.1f}, SARSA={sarsa:.1f})\")\n\nprint(\"\\n2. PHASE 2: N-Step Impact\")\nprint(\"-\" * 40)\nfor scenario in ['Basic', 'TakeCover', 'Deathmatch']:\n    n1 = nstep_compare[(nstep_compare['method'] == 'DQN_n1') & (nstep_compare['scenario'] == scenario)]['best_eval'].mean()\n    n3 = nstep_compare[(nstep_compare['method'] == 'DQN_n3') & (nstep_compare['scenario'] == scenario)]['best_eval'].mean()\n    if pd.notna(n1) and pd.notna(n3):\n        verdict = \"IMPROVED\" if n3 > n1 else \"DEGRADED\"\n        print(f\"   {scenario}: n=3 {verdict} ({n3-n1:+.1f})\")\n\nprint(\"\\n3. PHASE 3: Best Extension per Scenario\")\nprint(\"-\" * 40)\nfor scenario in ['Basic', 'TakeCover']:\n    scenario_data = all_compare[all_compare['scenario'] == scenario]\n    if len(scenario_data) > 0:\n        best = scenario_data.groupby('method')['best_eval'].mean().idxmax()\n        best_val = scenario_data.groupby('method')['best_eval'].mean().max()\n        print(f\"   {scenario}: {best} ({best_val:.1f})\")\n\nprint(\"\\n4. TOTAL TRAINING TIME\")\nprint(\"-\" * 40)\nprint(f\"   {df['training_hours'].sum():.1f} hours across {len(df)} experiments\")",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "---\n## 9. Export All Outputs to Drive\n\nSave all plots, tables, and analysis data to Drive for local sync."
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Export ALL outputs to Drive for local sync\n# =============================================================================\nimport shutil\n\n# Output directory on Drive\nOUTPUT_DIR = RESULTS_DIR / 'analysis_output'\nOUTPUT_DIR.mkdir(exist_ok=True)\n\nprint(\"=\" * 80)\nprint(\"EXPORTING ALL OUTPUTS TO DRIVE\")\nprint(\"=\" * 80)\n\n# 1. Save all plots to Drive\nplot_files = [\n    'phase1_learning_curves.png',\n    'phase2_learning_curves.png', \n    'phase3_learning_curves.png',\n    'all_methods_comparison.png'\n]\n\nprint(\"\\n1. Saving plots...\")\nfor fig_name in plot_files:\n    if Path(fig_name).exists():\n        shutil.copy(fig_name, OUTPUT_DIR / fig_name)\n        print(f\"   ✓ {fig_name}\")\n    else:\n        print(f\"   ✗ {fig_name} (not found)\")\n\n# 2. Save CSV to Drive\nprint(\"\\n2. Saving CSV...\")\nexport_df.to_csv(OUTPUT_DIR / 'all_results.csv', index=False)\nprint(f\"   ✓ all_results.csv ({len(export_df)} rows)\")\n\n# 3. Export complete analysis JSON for report writing\nprint(\"\\n3. Saving analysis JSON...\")\n\ndef safe_float(val):\n    \"\"\"Convert to float, handling NaN.\"\"\"\n    if pd.isna(val):\n        return None\n    return float(val)\n\nanalysis_export = {\n    'meta': {\n        'total_experiments': len(df),\n        'total_training_hours': safe_float(df['training_hours'].sum()),\n        'date_generated': pd.Timestamp.now().isoformat(),\n        'scenarios': list(df['scenario'].unique()),\n        'methods': list(df['method'].unique()),\n        'phases': list(df['phase'].unique()),\n    },\n    \n    # Phase 1: DQN vs Deep SARSA\n    'phase1': {},\n    \n    # Phase 2: N-step ablation\n    'phase2': {},\n    \n    # Phase 3: Extensions\n    'phase3': {},\n    \n    # Complete results table\n    'all_results': []\n}\n\n# Phase 1 detailed stats\nfor scenario in ['Basic', 'TakeCover', 'Deathmatch']:\n    analysis_export['phase1'][scenario] = {}\n    for method in ['DQN', 'DeepSARSA']:\n        data = phase1[(phase1['scenario'] == scenario) & (phase1['method'] == method)]\n        if len(data) > 0:\n            analysis_export['phase1'][scenario][method] = {\n                'best_eval_mean': safe_float(data['best_eval'].mean()),\n                'best_eval_std': safe_float(data['best_eval'].std()),\n                'final_reward_mean': safe_float(data['final_reward'].mean()),\n                'final_reward_std': safe_float(data['final_reward'].std()),\n                'training_hours_mean': safe_float(data['training_hours'].mean()),\n                'n_seeds': len(data)\n            }\n\n# Phase 2 stats\nfor scenario in ['Basic', 'TakeCover', 'Deathmatch']:\n    analysis_export['phase2'][scenario] = {}\n    for method_label, method_filter in [('n1', 'DQN_n1'), ('n3', 'DQN_n3')]:\n        data = nstep_compare[(nstep_compare['scenario'] == scenario) & (nstep_compare['method'] == method_filter)]\n        if len(data) > 0:\n            analysis_export['phase2'][scenario][method_label] = {\n                'best_eval_mean': safe_float(data['best_eval'].mean()),\n                'best_eval_std': safe_float(data['best_eval'].std()),\n                'n_seeds': len(data)\n            }\n\n# Phase 3 stats\nfor scenario in ['Basic', 'TakeCover']:\n    analysis_export['phase3'][scenario] = {}\n    for method in ['Baseline_DQN', 'DQN_PER', 'DDQN', 'Dueling_DDQN']:\n        data = all_compare[(all_compare['scenario'] == scenario) & (all_compare['method'] == method)]\n        if len(data) > 0:\n            analysis_export['phase3'][scenario][method] = {\n                'best_eval_mean': safe_float(data['best_eval'].mean()),\n                'best_eval_std': safe_float(data['best_eval'].std()),\n                'n_seeds': len(data)\n            }\n\n# All results as records\nfor _, row in export_df.iterrows():\n    analysis_export['all_results'].append({\n        'phase': row['phase'],\n        'method': row['method'],\n        'scenario': row['scenario'],\n        'seed': int(row['seed']) if pd.notna(row['seed']) else None,\n        'final_reward': safe_float(row['final_reward']),\n        'best_eval': safe_float(row['best_eval']),\n        'training_hours': safe_float(row['training_hours'])\n    })\n\n# Save JSON\njson_path = OUTPUT_DIR / 'analysis_complete.json'\nwith open(json_path, 'w') as f:\n    json.dump(analysis_export, f, indent=2)\nprint(f\"   ✓ analysis_complete.json\")\n\n# 4. Summary\nprint(\"\\n\" + \"=\" * 80)\nprint(\"EXPORT COMPLETE\")\nprint(\"=\" * 80)\nprint(f\"\\nAll outputs saved to: {OUTPUT_DIR}\")\nprint(f\"\\nLocal sync path (after Google Drive sync):\")\nprint(f\"   G:\\\\Drive'ım\\\\vizdoom-ablation-results\\\\analysis_output\\\\\")\nprint(f\"\\nFiles exported:\")\nprint(f\"   - 4 PNG plots (learning curves, bar charts)\")\nprint(f\"   - all_results.csv ({len(export_df)} experiments)\")\nprint(f\"   - analysis_complete.json (full analysis data)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}